This type of layer is the most important one in a convolutional neural network architecture and it's parameters focuses around a learnable \textbf{kernel}. Usually the size of a kernel, in spatial dimensionality is small, but they are spreading in the entire depth of the input. Moreover, when the input information meets a convolutional layer, it transforms the data into a 2D activation map by using filters over the spatial dimensionality of the data. Thus, every kernel has an activation map, that will be stacked in the entire depth dimension in order to form the full output size from a convolutional layer. In the end,\sout{s} a scalar product will be computed for every value in a kernel. \cite{IntroCNN, EvalPooling}.\par

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Images/exampleOfConvolution.png}
    \caption{Representation of a convolutional layer, where the midpoint element of the kernel is located over the input vector, of which is than computed end reinstaled with a weighted bulk of itself and every adjacent pixels \cite{IntroCNN}.}
    \label{fig:ex_conv}
\end{figure}

As shown in the Figure \ref{fig:ex_conv}, the network will learn kernels that 'heats' at a specific aspect for a given spatial point of the input. In other words, these are known as \textbf{activations} \cite{IntroCNN}. \par

Every neuron in a convolutional layer is connected to a modest area of the input volume  which is referred as the \textbf{receptive field size}, in order to reduce the complexity of the model. Moreover, the magnitude of the connectedness over the depth is roughly always equivalent to the input depth \cite{IntroCNN, EvalPooling}. \par 

In a convolutional layer there are three hyperparameters (\textbf{depth, stride} and \textbf{zero-padding}) which are used to optimise and reduce the complexity of the model \cite{IntroCNN}, they are:

\begin{enumerate}
    \item The first hyperparameter is  \textbf{depth}. This parameter can influence quite heavily the output of a convolutional layer. By reducing the depth will automatically minimize the overall number of neurons in the network, but sacrificing the capabilities of the model to recognize patterns \cite{IntroCNN}.
    \item The second hyperparameter, also used in pooling layers, is \textbf{stride}. The programmer can define the stride in which to set the depth over the dimensionality of the input data for placing the sensitive field. By setting a small value for the stride (e.g. 1) will massively overlap the sensitive field, generating very large activation. In contrast, when a large value is assigned to the stride will result in a reduced amount of overlapping, producing an output of decreased spatial dimensions \cite{IntroCNN}.
    \item Lastly there is the \textbf{zero-padding} hyperparameter and it's a straightforward process of padding the input borders. Thus, being an effective approach to give more control as to the size of the output numbers \cite{IntroCNN}.
\end{enumerate}

Using these techniques will result in altering the spatial dimensionalty of the output of convolutional layers. There is a formula to compute spatial dimensionalty \cite{IntroCNN}:

\begin{center}
    \begin{equation} 
        \frac{(V - R) + 2Z}{S + 1}
    \end{equation}
\end{center}

In the above formula, the variable \textbf{V} is the input volume size (\textit{$height\times width \times depth$}), \textbf{R} is the receptive/sensitive field size, \textbf{Z} represents the chunk of zero padding set and lastly \textbf{S} is the stride. If the result obtained is not a perfect integer without decimals, than the stride is not correctly set. Therefore the neurons will not be able to fit efficiently over the input \cite{IntroCNN, EvalPooling}. \par

\textbf{Parameter sharing} takes into account that if one region feature is convenient to calculate a dimensional region set, then it's feasible to work well in other regions \cite{IntroCNN}.   