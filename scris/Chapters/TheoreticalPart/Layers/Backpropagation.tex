In a convolutional neural network and also in a traditional ANN all layers will be trained with a backpropagation algorithm. The standard procedure is used in error propagation and weight variation in the following layers: fully connected, convolutional and subsampling layers. In the case of max pooling layers, the error is only propagated to a specific position using the Max pooling function, see Equation \ref{eq:max}. However, using overlapping pooling windows, it means storing more than one error signal in a single unit \cite{EvalPooling}.