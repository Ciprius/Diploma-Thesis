The aim of this type of layer is to reduce the resolution of the map features, where every such pooled feature coincides with the one from the previous layer. Thus, the input is combine in small $n \times n$ patch of units and the variable $n$ can have an arbitrary size \cite{IntroCNN,EvalPooling}. \par

There are two important and frequently used pooling operations: 
\begin{enumerate}
    \item \textbf{Subsampling function:} will take the sum of the inputs, multiplies it a scalar \ensuremath{\beta} after will add a bias \textit{b}, than the result will be passed over the non-linearity \cite{EvalPooling}, $a_j$ denotes the output and $a_i$ is the input of a pooling layer and N $\times$ N is the patch of the input image. It's function is the following:
        \begin{center}
            \begin{equation}
                a_j= tanh(\ensuremath{\beta} \sum_{N \times N} a_{i}^{\textit{$n\times n$}} + b)
            \end{equation}
        \end{center}
        
    \textcolor{green}{\sout{explain who are $a_i$ and $a_j$ and $N$ (big N)} \sout{and function $u$ (maybe an example)}}
    
    \item \textbf{Max pooling function:} will apply a so called window function \textit{u(x,y)}, which takes two arguments, on a input patch, after that calculates the maximum value in the neighborhood \cite{EvalPooling}, $a_j$ denotes the output and $a_i$ is the input of a pooling layer. It's function is the following.
        \begin{center}
            \begin{equation} \label{eq:max}
               a_j = \max(a_{i}^{\textit{$n\times n$}} u(n,n))
            \end{equation}
        \end{center}
\end{enumerate}
\par
In the end, regardless of what function is used, the result will be a feature map with a lower resolution. \par

The pooling layer in it's nature it's very destructive. Therefore, the stride and filters are usually set to \textit{$2\times2$}, allowing the layer to extend over the hole spatial size of the input. Moreover, there are two more types of pooling. The first one is  \textbf{overlapping pooling}, which uses a stride of 2 and a kernel which is set to 3. However, setting the kernel size to 3 will decrease the performance. The other type is the \textbf{general pooling}, being comprised of pooling neurons capable of performing a lot of common operations(i.e. L1/L2-normalization) \cite{IntroCNN}. 