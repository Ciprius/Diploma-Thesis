The most basic unit by which the nervous system works, in terms of processing is the neuron. They communicate with each others via synapses, by sending electrical signal (axon is the longer part which sends information to the dendrites of the other neuron). Through this connections the electrical signal is transmitted  across multiple neurons. The brain of a human is known to have a lot of neurons, approximately 100 billion neurons. If it is the case to reproduce this amount using existing computers, than it will be almost impossible to imitate this level of complexity. \textcolor{green}{oare stii de proiectul acesta: http://www.humanconnectomeproject.org/ ?}
As a reference a nematode has around 302 neurons and by using today technology would be easy to replicate this level of complexity \cite{ANNBasic}.\par

As stated above neurons receive, process and give an output data. However, the output isn't given at a constant rate, like an asynchronous call to get some data from the backend, unless it reaches a certain threshold. In other words, the function that does the above after a certain threshold is called an activation function \cite{ANNBasic}. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Images/exampleofthreshhold.png}
    \caption{A step and sigmoid function \cite{ANNBasic}}
    \label{fig:ex_threshold}
\end{figure}

As shown in Figure \ref{fig:ex_threshold}, the artificial neurons are very similar with the biological ones, in the way they respond to the stimuli. Moreover, in reality, artificial neural networks (ANNs) use , besides the action functions, some other functions called logistic functions. At the moment the most used function in the learning process is the sigmoid function, because is not so difficult to calculate in comparison with other functions. Also, in the course of chancing the weight values, the hole layer needs an activation function which can be differentiated \cite{ANNBasic}.The sigmoid function can be expressed as follows: 

\begin{center}
    \begin{equation}
        \ensuremath{\sigma}(x) =\frac{1}{1 + e^{-x}}
    \end{equation}
\end{center}

Both biological neurons and those from an artificial neural network can receive multiple inputs \cite{clinical}. The difference is the neurons in an artificial network need to add them in order to exceed the threshold and the processing is done using an activation function (i.e sigmoid function). Whereas a biological one simply process the given information \cite{NNandML, ANNbriefOverview}. The result processed by the activation function, becomes the output value for the next layer of neurons. As shown in the Figure \ref{fig:ex_input}, if the value resulted by adding the inputs A,B and C is greater than the threshold, this neuron will generate an output value  \cite{ANNBasic, MakeyourOwn}, as seen in the Figure \ref{fig:ex_input}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.4\linewidth]{Images/exampleOfInputOutput.png}
    \caption{Input and output of data from a neuron \cite{ANNBasic}}
    \label{fig:ex_input}
\end{figure}

Typically, in a biological neural network, neurons are located over several layers, and one neuron can have multiple connection with other neurons. The same architecture is applied in an artificial neural network, but with the following differences, the first layer, called the input layer, composed by input neurons which sends information through synapses to the second layer, called hidden layer (in this layer the programmer is unable to interpret how the result is obtained), and finally the third layer (output layer), which receives data from the second layer via more synapses \cite{MakeyourOwn, RealTime, ANNbriefOverview, NNandML}. \par

The learning process for an artificial neural network is done by updating some variables, such as the strength between the nodes, called weights, in order to obtain better matched output data to the target ones. \textcolor{green}{\sout{eu as evita sa folosesc termenul de accuracy pt ca nu e inca explicat/introdus si e oarecum specific problemelor de clasificare (ori ANN-urile rezolva si regressi); as zice ceva de genul: in order to obtained better matched/fitted output data to the target ones}} See Figure \ref{fig:ex_weight}, also a low weight value weakens the knowledge, whereas a high weight enhance it \cite{MakeyourOwn, NNandML, ANNbriefOverview, ClassifNN}.
\textcolor{green}{\sout{cred ca ar fi mai ok ca in loc de signal (termen oarecum specific NN reale, nu artificiale) sa zici information/knowledge/data}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/exampleOfWeights.png}
    \caption{Connection with weights between neurons of each layer \cite{ANNBasic}}
    \label{fig:ex_weight}
\end{figure}

As the Figure \ref{fig:ex_weight} shows, the weights (W1,2), (W1,3), (W2,3), (W2,2) and (W3,2) are accentuated by the strength of the signal thanks to a high weight value, also if the weight is equal to 0 than the signal is not transmitted. Updating the weights is done by determining the error between the predicted and correct output \cite{ANNBasic}. Thus, the error is splitted by the report of the weights on the links, then the errors obtained are backpropagated and reassembled \textcolor{red}{\sout{the error is obtained by dividing the error to the ratio of the weights on the links, than backpropagate and reassemble the divided errors}} \textcolor{green}{\sout{ceva nu e ok error is obtained by dividing the error?}}, see Figure \ref{fig:ex_backprog}. Moreover, in the case of a hierarchical structure, computing all the weights mathematically is extremely difficult and computational demanding. Thus, the use of the gradient descent method is very helpful \cite{ANNBasic, OnlineApprox, NNTricks}. The gradient descent method as a technique is used to find the minimum point by utilizing a cost function, which is way to determine how well a machine learning model has performed given the different values of each parameters. So its less important the starting value of the weight \textit{W}, because it will gradually alter it in order to reach the local minimum. \textcolor{green}{\sout{cine e costul? pana acum nu ai amintit de el si cum il poti determina. ai vb doar de erorare}} Thus, without expensive mathematical computations, the gradient minimizes the error between the predicted and output value \cite{ANNBasic}.

\textcolor{green}{ar merita aici si o mica discutie despre loss si regularisation - ca tradeoff intre eroare buna si complexitatea modelului invatat de un ANN}

So this is the process with which an artificial neural network learns.

\textcolor{green}{poti enumera si de alti algoritmi de identificare a weight-urilor optime (Evol alg, Markov random fields, other optimization methods)}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Images/exampleOfBackPropagation.png}
    \caption{Backpropagation of the error in order to update the weights \cite{ANNBasic}}
    \label{fig:ex_backprog}
\end{figure}

\textcolor{green}{\sout{asa cum am zis deja, eu as vb despre supervised si unsupervised inainte de povestea cu ANN pt ca poti folosi si alti alg, nu doar ANN, pt a rezolva o problema de supervis}
}